{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neuroevolution v3.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"XwdAwywX3zcQ","colab_type":"text"},"cell_type":"markdown","source":["To get these results, we used as a basis the blogpost in : https://towardsdatascience.com/reinforcement-learning-without-gradients-evolving-agents-using-genetic-algorithms-8685817d84f and the associated github : https://github.com/paraschopra/deepneuroevolution/blob/master/openai-gym-cartpole-neuroevolution.ipynb\n","\n"]},{"metadata":{"id":"21kBNv79lsyE","colab_type":"text"},"cell_type":"markdown","source":["# Import Useful Libraries"]},{"metadata":{"id":"sgZvmUawSqJ0","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","import copy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CiSiV0yzSqJ5","colab_type":"code","colab":{}},"cell_type":"code","source":["#gym library for Reinforcement Leaning Agents\n","import gym\n","from gym.wrappers import Monitor"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VWNo2iBBSqJ8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"ab8623c1-7386-4e72-8b09-f2863c6be8dd","executionInfo":{"status":"ok","timestamp":1553196801659,"user_tz":-60,"elapsed":529,"user":{"displayName":"Meryem BEN-GOUMI","photoUrl":"","userId":"03183206500396027684"}}},"cell_type":"code","source":["#Pytorch Library for Deep Reinforcement Learning Implementation\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","#disable gradients as we will not use them\n","torch.set_grad_enabled(False)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7f95d8f70198>"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"ckyVkJOpne3R","colab_type":"text"},"cell_type":"markdown","source":["# Initialization: Environment Set-up and Random Generation of Agents"]},{"metadata":{"id":"1FZqISUphfCO","colab_type":"text"},"cell_type":"markdown","source":["We start by setting the environment and hyperparameters below. We want to generate 300 agents to get the initial state.\n","\n"]},{"metadata":{"id":"niaPUY7XuYGf","colab_type":"code","colab":{}},"cell_type":"code","source":["class CartPoleAI(nn.Module):\n","        def __init__(self):\n","            super().__init__()\n","            self.fc = nn.Sequential(\n","                        nn.Linear(4,32, bias=True),\n","                        nn.ReLU(),\n","                        nn.Linear(32,2, bias=True),\n","                        nn.Softmax(dim=1)\n","                        )\n","\n","                \n","        def forward(self, inputs):\n","            x = self.fc(inputs)\n","            return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3STlukJExFAU","colab_type":"code","colab":{}},"cell_type":"code","source":["nbre_actions = 2 #movement to the left or right"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NqmvOYATuYGy","colab_type":"code","colab":{}},"cell_type":"code","source":["def return_random_agents(num_agents):\n","    \n","    agents = []\n","    for _ in range(num_agents):\n","        \n","        agent = CartPoleAI()\n","        \n","        for param in agent.parameters():\n","            param.requires_grad = False\n","            \n","        init_weights(agent)\n","        agents.append(agent)\n","        \n","        \n","    return agents\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"_r8nGYr6vSRh","colab_type":"code","colab":{}},"cell_type":"code","source":["agents = return_random_agents(300)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bW2PxRA3wKiP","colab_type":"text"},"cell_type":"markdown","source":["# Computing the Rewards (fitness function)"]},{"metadata":{"id":"iwNyvNHswOSm","colab_type":"text"},"cell_type":"markdown","source":["The functions below run the agents n times and compute the resulting reward."]},{"metadata":{"id":"vBxMOJWeuYHF","colab_type":"code","colab":{}},"cell_type":"code","source":["def run_agents(agents):\n","    \n","    reward_agents = []\n","    env = gym.make(\"CartPole-v0\")\n","    \n","    for agent in agents:\n","        agent.eval()\n","    \n","        observation = env.reset()\n","        \n","        r=0\n","        \n","        for _ in range(250):\n","            \n","            state_observed = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n","            choice_proba = agent(state_observed).detach().numpy()[0]\n","            next_action = np.random.choice(range(nbre_actions), 1, p=choice_proba).item()\n","            new_observation, reward, done, info = env.step(next_action)\n","            r=r+reward\n","            \n","            observation = new_observation\n","\n","            if(done):\n","                break\n","\n","        reward_agents.append(r)        \n","        #reward_agents.append(s)\n","        \n","    \n","    return reward_agents"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LeW8rFLzuYHT","colab_type":"code","colab":{}},"cell_type":"code","source":["def return_average_score(agent, runs):\n","    score = 0.\n","    for i in range(runs):\n","        score += run_agents([agent])[0]\n","    return score/runs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SE4gXtZruYHy","colab_type":"code","colab":{}},"cell_type":"code","source":["def run_agents_n_times(agents, runs):\n","    avg_score = []\n","    for agent in agents:\n","        avg_score.append(return_average_score(agent,runs))\n","    return avg_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qhzxNG2vuYIb","colab_type":"code","colab":{}},"cell_type":"code","source":["def mutate(agent):\n","\n","    child_agent = copy.deepcopy(agent)\n","    \n","    mutation_power = 0.02\n","    \n","    for param in child_agent.parameters():\n","    \n","        if(len(param.shape)==4): #weights of Conv2D\n","\n","            for i0 in range(param.shape[0]):\n","                for i1 in range(param.shape[1]):\n","                    for i2 in range(param.shape[2]):\n","                        for i3 in range(param.shape[3]):\n","                            \n","                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n","                                \n","                                    \n","\n","        elif(len(param.shape)==2): #weights of linear layer\n","            for i0 in range(param.shape[0]):\n","                for i1 in range(param.shape[1]):\n","                    \n","                    param[i0][i1]+= mutation_power * np.random.randn()\n","                        \n","\n","        elif(len(param.shape)==1): #biases of linear layer or conv layer\n","            for i0 in range(param.shape[0]):\n","                \n","                param[i0]+=mutation_power * np.random.randn()\n","\n","    return child_agent"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JRwlpp5vuYIk","colab_type":"code","colab":{}},"cell_type":"code","source":["def return_children(agents, sorted_parent_indexes, elite_index):\n","    \n","    children_agents = []\n","    \n","    #first take selected parents from sorted_parent_indexes and generate N-1 children\n","    for i in range(len(agents)-1):\n","        \n","        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n","        children_agents.append(mutate(agents[selected_agent_index]))\n","\n","    #now add one elite\n","    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n","    children_agents.append(elite_child)\n","    elite_index=len(children_agents)-1 #it is the last one\n","    \n","    return children_agents, elite_index"],"execution_count":0,"outputs":[]},{"metadata":{"id":"41VfbjbzuYIw","colab_type":"code","colab":{}},"cell_type":"code","source":["def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n","    \n","    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n","    \n","    if(elite_index is not None):\n","        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n","        \n","    top_score = None\n","    top_elite_index = None\n","    \n","    for i in candidate_elite_index:\n","        score = return_average_score(agents[i],runs=5)\n","        \n","        if(top_score is None):\n","            top_score = score\n","            top_elite_index = i\n","        elif(score > top_score):\n","            top_score = score\n","            top_elite_index = i\n","            \n","    \n","    child_agent = copy.deepcopy(agents[top_elite_index])\n","    return child_agent\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"eM_FyFFauYJM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1735},"outputId":"171580bd-ccc9-4678-b768-7c809eebbb39","executionInfo":{"status":"error","timestamp":1553197522068,"user_tz":-60,"elapsed":105580,"user":{"displayName":"Meryem BEN-GOUMI","photoUrl":"","userId":"03183206500396027684"}}},"cell_type":"code","source":["# Number of agents to consider as the Fittest for Natural Selection\n","number_fittest = 15\n","\n","# run evolution until X generations (the results are improved with 1000 generations)\n","generations = 100\n","\n","elite_index = None\n","\n","for generation in range(generations):\n","\n","    # return rewards of agents\n","    rewards = run_agents_n_times(agents, 3) #return average of 3 runs\n","\n","    # sort by rewards\n","    sorted_parent_indexes = np.argsort(rewards)[::-1][:number_fittest] #reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n","\n","    \n","    top_rewards = []\n","    for best_parent in sorted_parent_indexes:\n","        top_rewards.append(rewards[best_parent])\n","    \n","    print(\"### Generation number {} ### \".format(generation))\n","    print(\"Rewards for the top {} fittest agents: {}\".format(number_fittest, top_rewards))\n","    print(\"mean reward is {}\".format(np.mean(np.array(top_rewards))))\n","    \n","    # setup an empty list for containing children agents\n","    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n","\n","    # kill all agents, and replace them with their children\n","    agents = children_agents"],"execution_count":52,"outputs":[{"output_type":"stream","text":["### Generation number 0 ### \n","Rewards for the top 15 fittest agents: [70.0, 69.66666666666667, 63.0, 62.333333333333336, 62.0, 56.666666666666664, 56.666666666666664, 56.333333333333336, 56.333333333333336, 56.0, 55.666666666666664, 55.0, 54.666666666666664, 54.666666666666664, 52.333333333333336]\n","mean reward is 58.75555555555555\n","### Generation number 1 ### \n","Rewards for the top 15 fittest agents: [85.0, 67.0, 67.0, 66.33333333333333, 66.33333333333333, 61.666666666666664, 61.333333333333336, 60.666666666666664, 60.333333333333336, 59.0, 58.0, 58.0, 57.666666666666664, 56.666666666666664, 55.666666666666664]\n","mean reward is 62.7111111111111\n","### Generation number 2 ### \n","Rewards for the top 15 fittest agents: [68.66666666666667, 66.0, 64.0, 63.0, 63.0, 59.333333333333336, 59.333333333333336, 58.666666666666664, 56.666666666666664, 56.0, 56.0, 55.666666666666664, 55.333333333333336, 54.666666666666664, 54.666666666666664]\n","mean reward is 59.39999999999999\n","### Generation number 3 ### \n","Rewards for the top 15 fittest agents: [81.0, 74.0, 70.33333333333333, 65.33333333333333, 62.0, 61.666666666666664, 61.0, 59.333333333333336, 57.666666666666664, 57.666666666666664, 57.666666666666664, 57.666666666666664, 57.333333333333336, 56.666666666666664, 56.333333333333336]\n","mean reward is 62.377777777777766\n","### Generation number 4 ### \n","Rewards for the top 15 fittest agents: [84.66666666666667, 76.33333333333333, 71.33333333333333, 69.0, 68.0, 67.0, 65.33333333333333, 65.33333333333333, 65.0, 64.0, 62.0, 62.0, 61.0, 61.0, 61.0]\n","mean reward is 66.86666666666666\n","### Generation number 5 ### \n","Rewards for the top 15 fittest agents: [77.33333333333333, 76.33333333333333, 74.0, 69.33333333333333, 66.66666666666667, 66.0, 64.33333333333333, 62.666666666666664, 59.666666666666664, 57.666666666666664, 57.333333333333336, 56.0, 55.666666666666664, 54.333333333333336, 54.0]\n","mean reward is 63.422222222222224\n","### Generation number 6 ### \n","Rewards for the top 15 fittest agents: [76.66666666666667, 76.33333333333333, 70.66666666666667, 70.33333333333333, 69.66666666666667, 68.66666666666667, 67.66666666666667, 67.33333333333333, 67.0, 65.0, 64.0, 63.333333333333336, 63.333333333333336, 60.666666666666664, 60.0]\n","mean reward is 67.37777777777778\n","### Generation number 7 ### \n","Rewards for the top 15 fittest agents: [85.0, 77.33333333333333, 76.0, 74.33333333333333, 72.66666666666667, 71.66666666666667, 70.33333333333333, 68.66666666666667, 68.66666666666667, 66.66666666666667, 66.0, 65.66666666666667, 65.33333333333333, 65.33333333333333, 63.666666666666664]\n","mean reward is 70.48888888888888\n","### Generation number 8 ### \n","Rewards for the top 15 fittest agents: [88.0, 88.0, 87.33333333333333, 83.66666666666667, 83.33333333333333, 81.0, 73.66666666666667, 69.0, 68.66666666666667, 68.66666666666667, 67.0, 65.66666666666667, 64.66666666666667, 64.0, 63.333333333333336]\n","mean reward is 74.39999999999999\n","### Generation number 9 ### \n","Rewards for the top 15 fittest agents: [114.33333333333333, 82.0, 77.33333333333333, 76.0, 75.0, 74.66666666666667, 74.33333333333333, 74.33333333333333, 73.0, 70.0, 67.33333333333333, 67.33333333333333, 67.33333333333333, 65.33333333333333, 65.33333333333333]\n","mean reward is 74.91111111111111\n","### Generation number 10 ### \n","Rewards for the top 15 fittest agents: [88.33333333333333, 87.66666666666667, 80.0, 78.33333333333333, 75.66666666666667, 68.33333333333333, 67.33333333333333, 66.66666666666667, 66.33333333333333, 66.0, 64.33333333333333, 63.666666666666664, 63.666666666666664, 63.333333333333336, 61.333333333333336]\n","mean reward is 70.73333333333333\n","### Generation number 11 ### \n","Rewards for the top 15 fittest agents: [91.0, 86.0, 80.66666666666667, 78.66666666666667, 77.33333333333333, 76.66666666666667, 76.0, 71.66666666666667, 71.33333333333333, 71.0, 69.0, 68.33333333333333, 68.33333333333333, 68.33333333333333, 68.33333333333333]\n","mean reward is 74.84444444444445\n","### Generation number 12 ### \n","Rewards for the top 15 fittest agents: [103.0, 97.66666666666667, 76.0, 76.0, 75.33333333333333, 74.0, 73.66666666666667, 72.0, 71.33333333333333, 69.66666666666667, 68.66666666666667, 68.33333333333333, 66.0, 65.66666666666667, 65.33333333333333]\n","mean reward is 74.84444444444445\n","### Generation number 13 ### \n","Rewards for the top 15 fittest agents: [79.33333333333333, 74.33333333333333, 73.0, 70.66666666666667, 68.66666666666667, 68.33333333333333, 68.33333333333333, 66.33333333333333, 66.33333333333333, 64.0, 63.666666666666664, 63.0, 61.666666666666664, 61.333333333333336, 61.333333333333336]\n","mean reward is 67.35555555555555\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m             \u001b[0mprod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'prod'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-2075ee2b24a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# return rewards of agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_agents_n_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#return average of 3 runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# sort by rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-00889bc3cebb>\u001b[0m in \u001b[0;36mrun_agents_n_times\u001b[0;34m(agents, runs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mavg_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mavg_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_average_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavg_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-d0bf248736d3>\u001b[0m in \u001b[0;36mreturn_average_score\u001b[0;34m(agent, runs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrun_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-d0ab96e2beaf>\u001b[0m in \u001b[0;36mrun_agents\u001b[0;34m(agents)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mstate_observed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mchoice_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_observed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbre_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchoice_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mnew_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m             \u001b[0mprod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}